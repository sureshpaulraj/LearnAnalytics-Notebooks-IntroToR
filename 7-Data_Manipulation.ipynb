{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    "\n",
    "In this module, we will explore how to effectively do data manipulation in R.\n",
    "\n",
    "If you are unfamiliar with notebooks, please review some basics [here](https://github.com/michhar/useR2016-tutorial-jupyter). \n",
    "\n",
    "## Essential Tips\n",
    "\n",
    "A very brief summary of the critical components and commands within jupyter are:\n",
    "\n",
    "1. Critically, press `Ctrl+Enter` to run (or render) the current cell.\n",
    "2. Output will print to the notebook. You may have to scroll up to see it all.\n",
    "3. Get help for any function by typing a question mark and then its name into\n",
    "   the console: `?rxLinMod`. It will split the window, and will bring up the documentation for \n",
    "   that function below.\n",
    "5. Files will appear in the specified directory. You can find them by selecting File in the menu bar and selecting \"Open...\". This will open a new browser window with a file navigator.\n",
    "6. R objects can be viewed by typing `ls()` in an R cell.\n",
    "7. Run all the example code!\n",
    "\n",
    "There are a number of hands-on exercises in the document, so while you can run the notebook from beginning to end, you will get a lot more out of it by actually walking through cell-by-cell, and filling out the corresponding exercises.\n",
    "\n",
    "These notebooks are based on a tutorial presented at a Microsoft conference in June of 2016. The original files are available [here](https://github.com/joseph-rickert/MLADS_JUNE_2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Data\n",
    "\n",
    "The biggest part of a data analysis project is preparing the data. This includes cleaning the data, deciding which variables to use, dealing with missing values and just wrestling the data into a form that is useful. Hadley Wickham, a statistician and prolific R package deeveloper, has addressed this process of wrangling data on conceptual level with his notion of \"tidy data\" and has put this concept into play with a series of packages that simplify the various processes of data manipulation. The examples in this module are based on using functions in Hadley's dplyr package. First a definition of Tidy Data from Hadley's JSS paper that you can find here:http://vita.had.co.nz/papers/tidy-data.pdf\n",
    "\n",
    "Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is\n",
    "messy or tidy depending on how rows, columns and tables are matched up with observations,\n",
    "variables and types. \n",
    "\n",
    "In tidy data: \n",
    "\n",
    "1. each variable forms a column, \n",
    "2. each observation forms a row, and \n",
    "3. each type of observational unit forms a table.\n",
    "\n",
    "The two most relevant packages for helping with this front are `dplyr` and `tidyr`, both of which we'll explore momentarily.\n",
    "\n",
    "Before we get started, though, let's source a configuration file in the next cell. It simply makes sure that the relevant R packages and datasets are available. You do not need to look at it, but if you are interested, you can view the configuration file [here](Resources/config.R). As a side note, it also sets the `stringsAsFactors` option to FALSE, so when we read in character variables, they won't automatically turn to factors. \n",
    "\n",
    "It may take a few moments to run the first time you run it, but it should be fast afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source(\"Resources/config.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for this Module\n",
    "In this module we will use two data sets: the IBM stock data from the module on [reading data into R](2-Reading_csv_files.ipynb) and another data set from Yahoo Finance on IBM dividend data.\n",
    "\n",
    "Let's go ahead and fetch the data to a local file if we don't already have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir <- 'data'\n",
    "fileName <- \"ibm_stock.csv\"\n",
    "file <- file.path(data_dir, fileName)\n",
    "if(file.exists(file)){\n",
    "    cat(\"*** stock file already exists. Loading local copy.\\n\")\n",
    "    ibm_stock <- read.csv(file, header = TRUE)\n",
    "}else{\n",
    "    cat(\"*** Reading stock file from remote URL.\\n\")\n",
    "    url <- \"http://real-chart.finance.yahoo.com/table.csv?s=IBM&d=1&e=1&f=2016&g=d&a=0&b=2&c=1962&ignore=.csv\"\n",
    "    ibm_stock <- read.csv(url,header=TRUE,sep=\",\")\n",
    "    cat(\"*** Saving stock file locally for use later.\\n\")\n",
    "    write.csv(ibm_stock, file = file)\n",
    "}\n",
    "head(ibm_stock, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the other file\n",
    "\n",
    "Now, let's get the additional file and save it locally as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file2 <- file.path(data_dir, 'ibm_div.csv')\n",
    "if(file.exists(file2)){\n",
    "   cat('*** dividend file already exists locally, reading from there!\\n')\n",
    "   ibm_div <- read.csv(file2, header = TRUE)\n",
    "}else{\n",
    "    cat(\"*** reading dividend file from remote URL...\\n\")\n",
    "    url2 <- \"http://real-chart.finance.yahoo.com/table.csv?s=IBM&a=00&b=2&c=1962&d=01&e=6&f=2016&g=v&ignore=.csv\"\n",
    "    ibm_div <- read.csv(url2,header=TRUE)\n",
    "    cat(\"*** saving dividend data locally for use later...\\n\")\n",
    "    write.csv(ibm_div, file = file2)\n",
    "}\n",
    "head(ibm_div, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting a Data Frame\n",
    "\n",
    "The first function from dplyr that we will play with is the `mutate()` function. The `mutate()` function is very useful as it allows us to create an arbitrary number of transformations by just specifying them inline. In this case, we do two things:\n",
    "\n",
    "- we create a measure `Volatility` that measures how much difference there was within the day as a function of its opening price\n",
    "- we cast the Date variable so that it is a propert date variable, rather than just a character string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "class(ibm_stock$Date)\n",
    "ibm_stock <- mutate(ibm_stock, Volatility = (High - Low)/Open,\n",
    "                   Date = as.Date(Date))\n",
    "class(ibm_stock$Date)\n",
    "head(ibm_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting rows of a Data Frame\n",
    "\n",
    "The `filter()` function allows us to subset rows. We've previously seen the use of row indexing using square brackets, but `filter()` can be a little easier to read.\n",
    "\n",
    "Here we create a new data frame contianing IBM stock data that is on or later than 1/1/2000. We do this by filtering out the rows with older data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IBMge2000 <- filter(ibm_stock, Date >= as.Date('2000-01-01'))\n",
    "head(IBMge2000)\n",
    "tail(IBMge2000)\n",
    "max(IBMge2000$Date)\n",
    "min(IBMge2000$Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating Data\n",
    "\n",
    "Here we aggragate the daily stock data in our original IBM data frame into monthly data. The first step we will do is to use `mutate()` (in combination with the lubridate package) to create new variables that represent the month and year of each observation. Then we will create a grouped data frame, and then we will use the `summarise()` function to actually aggregate for each of the levels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(lubridate)\n",
    "ibm_stock <- mutate(ibm_stock,\n",
    "              Month = month(Date), # Add variable Month\n",
    "              Year  = year(Date))  # Add variable Year \n",
    "\n",
    "head(ibm_stock,2)\n",
    "sapply(ibm_stock,class) # look at what type the variables are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we group the data using the `group_by()` function, and then use the `summarise()` function to roll up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "by_yr_mo <- group_by(ibm_stock,Year,Month) # denote the grouping variables\n",
    "head(by_yr_mo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now actually roll up and summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "ibm_a <- summarise(by_yr_mo,\n",
    "                       count = n(),\n",
    "                       m.Open = mean(Open, na.rm = TRUE),\n",
    "                       m.High = mean(High, na.rm = TRUE),\n",
    "                       m.Low  = mean(Low, na.rm = TRUE),\n",
    "                       m.Close = mean(Close, na.rm = TRUE),\n",
    "                       m.Volume = mean(Volume, na.rm = TRUE),\n",
    "                       m.Adj.Close = mean(Adj.Close, na.rm = TRUE),\n",
    "                       m.Volatility = mean(Volatility, na.rm = TRUE))\n",
    "\n",
    "head(ibm_a); \n",
    "tail(ibm_a); \n",
    "dim(ibm_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Operations\n",
    "\n",
    "Now, a lot of the power in dplyr actually comes from creating workflows that allow seamless concatenation of multiple steps. This is particularly powerful when we leverage the pipe operate from the magrittr package (which is loaded when dplyr is loaded). For instance, we can load the data, do mutations, filter, group data frames and summarize in a single workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read.csv(file, header = TRUE) %>%\n",
    " mutate(Volatility = (High - Low)/Open,\n",
    "        Date = as.Date(Date),\n",
    "        Month = month(Date),\n",
    "        Year = year(Date)) %>%\n",
    " filter(Year > 1999) %>%\n",
    " group_by(Year, Month) %>%\n",
    " summarise(count = n(), m.Vol = mean(Volatility), sd.Vol = sd(Volatility)) -> stock_agg_by_yr_mo\n",
    "head(stock_agg_by_yr_mo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional aggregations\n",
    "\n",
    "If we try to aggregate on an already aggregated data frame, then we will systematically knock off the lowest levels of the grouping variables. In this case, we will aggregated over Months if we aggregate `stock_agg_by_yr_mo`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summarise(stock_agg_by_yr_mo, \n",
    "          ndays_contributing = sum(count), \n",
    "          n = n(), \n",
    "          m.m.Vol = mean(m.Vol), \n",
    "          m.sd.Vol = mean(sd.Vol)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data Frames\n",
    "\n",
    "In this section we merge the IBM stock file with the IBM dividend file and create a new data frame to hold the merged data. We do a \"right join\" which will keep all of the rows in the IBM_div data frame and only include rows from the IBM stock data frame with dates that match a dividend date. This new data frame includes the dividends and stock pricies on the days the dividends were paid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ibm_div <- mutate(ibm_div, Date = as.Date(Date)) # Make Date into a proper date data type\n",
    "class(ibm_div$Date)\n",
    "ibm2 <- right_join(ibm_stock,ibm_div,by=\"Date\")        # Merge the data \n",
    "head(ibm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "We can also use the `arrange()` function from dplyr to facilitate sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(ibm2,2)\n",
    "ibm2 <- arrange(ibm2, Date)                # Sort by date in ascending order\n",
    "head(ibm2,2)\n",
    "ibm2 <- arrange(ibm2, desc(Date))                # Sort by date in descending order\n",
    "head(ibm2,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping a Data Frame\n",
    "\n",
    "One often needs to reshape a data frame from either long format to wide format or the other way around. What long and wide mean is just illustrated by example, but basically wide format is what you might see in an Excel spreadsheet and long format is generally what statisticians want: one row for every combination of data.\n",
    "\n",
    "To illustrate these transformations we will use a subset of the aggregated IBM data.\n",
    "\n",
    "Specifically, we will just extract the `Year`, `Month`, and average closing price (`m.Close`) from the aggregated data using the `select()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ibm_subset <- select(ibm_a, Year, Month, m.Close)\n",
    "head(ibm_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is in long format, so we will reshape it into wide format. It is in long format because each observation has a single row. We will reshape it into wide format using functions from the `tidyr` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(tidyr)\n",
    "ibm_wide <- spread(ibm_subset, key = Month, value = m.Close)\n",
    "head(ibm_wide,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we provide names for the columns. Note that R has a built in vector that gives abbreviated names for the months. R has many such convenience variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names(ibm_wide)[2:13] <- month.abb    \n",
    "head(ibm_wide,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wll go back to long format and compare the new long format data frame with what we started out with. We will use the `gather()` function to do this. Note that the fourth argument interprets the `:` operator as the sequence of column names between `Jan` and `Dec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ibm_long <- gather(ibm_wide,key = Month,value = Close, Jan:Dec)\n",
    "ibm_long <- arrange(ibm_long,Year,Month)          # Sort the data frame\n",
    "head(ibm_long,3)                                  # New long format data frame\n",
    "head(ibm_subset, 3)                               # What we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on these spread() and arrange() functions see the following tutorial:\n",
    "http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "MR Client 3.2.2",
   "language": "R",
   "name": "irmrc32"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
